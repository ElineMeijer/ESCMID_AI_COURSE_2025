{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score, \n",
    "    confusion_matrix, classification_report, roc_curve, auc, \n",
    "    precision_recall_curve, average_precision_score,\n",
    "    mean_squared_error, mean_absolute_error, r2_score,\n",
    "    log_loss, cohen_kappa_score, matthews_corrcoef\n",
    ")\n",
    "from sklearn.preprocessing import label_binarize\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Table of Contents\n",
    "\n",
    "# 1. **Binary Classification Metrics**\n",
    "- Confusion Matrix\n",
    "- Accuracy, Precision, Recall, F1-Score\n",
    "- ROC Curve and AUC\n",
    "- Precision-Recall Curve\n",
    "- Log Loss\n",
    "- Matthews Correlation Coefficient\n",
    " \n",
    "# 2. **Multi-class Classification Metrics**\n",
    "- Multi-class Confusion Matrix\n",
    "- Macro vs Micro Averaging\n",
    "- Cohen's Kappa\n",
    " \n",
    "# 3. **Regression Metrics**\n",
    "- Mean Squared Error (MSE)\n",
    "- Root Mean Squared Error (RMSE)\n",
    "- Mean Absolute Error (MAE)\n",
    "- R-squared (R²)\n",
    "- Mean Absolute Percentage Error (MAPE)\n",
    " \n",
    "# 4. **Additional  Topics**\n",
    "- Class Imbalance Considerations\n",
    "- Metric Selection Guidelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# =============================================================================\n",
    "# 1. BINARY CLASSIFICATION METRICS\n",
    "# ============================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n BINARY CLASSIFICATION METRICS\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Generate fake binary classification data\n",
    "n_samples = 10\n",
    "\n",
    "# True labels (0 or 1)\n",
    "y_true_binary = np.random.binomial(1, 0.3, n_samples)  # 30% positive class\n",
    "\n",
    "# Predicted probabilities (simulating a decent model)\n",
    "y_prob_binary = np.random.beta(2, 5, n_samples)  # Base probabilities\n",
    "y_prob_binary[y_true_binary == 1] += np.random.normal(0.4, 0.1, sum(y_true_binary))  # Boost positive class probs\n",
    "y_prob_binary = np.clip(y_prob_binary, 0, 1)  # Ensure [0,1] range\n",
    "\n",
    "# Predicted labels (using 0.5 threshold)\n",
    "y_pred_binary = (y_prob_binary >= 0.5).astype(int)\n",
    "\n",
    "print(f\"Dataset: {n_samples} samples\")\n",
    "print(f\"True positive class ratio: {y_true_binary.mean():.3f}\")\n",
    "print(f\"Predicted positive class ratio: {y_pred_binary.mean():.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.1 Confusion Matrix\n",
    "print(\"\\n Confusion Matrix\")\n",
    "print(\"-\" * 20)\n",
    "\n",
    "cm = confusion_matrix(y_true_binary, y_pred_binary)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(f\"                 Predicted\")\n",
    "print(f\"                 0    1\")\n",
    "print(f\"Actual    0   {cm[0,0]:4d} {cm[0,1]:4d}\")\n",
    "print(f\"          1   {cm[1,0]:4d} {cm[1,1]:4d}\")\n",
    "\n",
    "# Extract values for clarity\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "print(f\"\\nTrue Negatives (TN):  {tn}\")\n",
    "print(f\"False Positives (FP): {fp}\")\n",
    "print(f\"False Negatives (FN): {fn}\")\n",
    "print(f\"True Positives (TP):  {tp}\")\n",
    "\n",
    "# Visualize confusion matrix\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.ylabel('True Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "\n",
    "# %%\n",
    "# 1.2 Basic Classification Metrics\n",
    "print(\"\\n Basic Classification Metrics\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "accuracy = accuracy_score(y_true_binary, y_pred_binary)\n",
    "precision = precision_score(y_true_binary, y_pred_binary)\n",
    "recall = recall_score(y_true_binary, y_pred_binary)\n",
    "f1 = f1_score(y_true_binary, y_pred_binary)\n",
    "\n",
    "print(f\"Accuracy:  {accuracy:.4f} = (TP + TN) / (TP + TN + FP + FN)\")\n",
    "print(f\"           {accuracy:.4f} = ({tp} + {tn}) / ({tp} + {tn} + {fp} + {fn})\")\n",
    "\n",
    "print(f\"\\nPrecision: {precision:.4f} = TP / (TP + FP)\")\n",
    "print(f\"           {precision:.4f} = {tp} / ({tp} + {fp})\")\n",
    "print(\"           → Of all positive predictions, how many were correct?\")\n",
    "\n",
    "print(f\"\\nRecall:    {recall:.4f} = TP / (TP + FN)\")\n",
    "print(f\"           {recall:.4f} = {tp} / ({tp} + {fn})\")\n",
    "print(\"           → Of all actual positives, how many were found?\")\n",
    "\n",
    "print(f\"\\nF1-Score:  {f1:.4f} = 2 × (Precision × Recall) / (Precision + Recall)\")\n",
    "print(f\"           {f1:.4f} = 2 × ({precision:.4f} × {recall:.4f}) / ({precision:.4f} + {recall:.4f})\")\n",
    "print(\"           → Harmonic mean of precision and recall\")\n",
    "\n",
    "# %%\n",
    "# 1.3 ROC Curve and AUC\n",
    "print(\"\\n ROC Curve and AUC\")\n",
    "print(\"-\" * 20)\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(y_true_binary, y_prob_binary)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "print(f\"AUC-ROC: {roc_auc:.4f}\")\n",
    "print(\"AUC-ROC measures the area under the ROC curve\")\n",
    "print(\"Perfect classifier: AUC = 1.0\")\n",
    "print(\"Random classifier: AUC = 0.5\")\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.plot(fpr, tpr, color='blue', lw=2, label=f'ROC Curve (AUC = {roc_auc:.3f})')\n",
    "plt.plot([0, 1], [0, 1], color='red', lw=1, linestyle='--', label='Random Classifier')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate (FPR)')\n",
    "plt.ylabel('True Positive Rate (TPR)')\n",
    "plt.title('ROC Curve')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# %%\n",
    "# 1.4 Precision-Recall Curve\n",
    "print(\"\\n Precision-Recall Curve\")\n",
    "print(\"-\" * 25)\n",
    "\n",
    "precision_vals, recall_vals, pr_thresholds = precision_recall_curve(y_true_binary, y_prob_binary)\n",
    "avg_precision = average_precision_score(y_true_binary, y_prob_binary)\n",
    "\n",
    "print(f\"Average Precision (AP): {avg_precision:.4f}\")\n",
    "print(\"AP summarizes the precision-recall curve\")\n",
    "print(\"Especially useful for imbalanced datasets\")\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.plot(recall_vals, precision_vals, color='green', lw=2, label=f'PR Curve (AP = {avg_precision:.3f})')\n",
    "plt.axhline(y=y_true_binary.mean(), color='red', linestyle='--', \n",
    "           label=f'Random Classifier (AP = {y_true_binary.mean():.3f})')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Precision-Recall Curve')\n",
    "plt.legend(loc=\"lower left\")\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# %%\n",
    "# 1.5 Log Loss (Cross-Entropy Loss)\n",
    "print(\"\\n Log Loss (Cross-Entropy)\")\n",
    "print(\"-\" * 25)\n",
    "\n",
    "logloss = log_loss(y_true_binary, y_prob_binary)\n",
    "print(f\"Log Loss: {logloss:.4f}\")\n",
    "print(\"Log Loss penalizes confident wrong predictions heavily\")\n",
    "print(\"Lower is better (perfect = 0)\")\n",
    "print(\"Formula: -[y*log(p) + (1-y)*log(1-p)]\")\n",
    "\n",
    "# %%\n",
    "# 1.6 Matthews Correlation Coefficient\n",
    "print(\"\\n Matthews Correlation Coefficient (MCC)\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "mcc = matthews_corrcoef(y_true_binary, y_pred_binary)\n",
    "print(f\"MCC: {mcc:.4f}\")\n",
    "print(\"MCC ranges from -1 to +1\")\n",
    "print(\"+1: Perfect prediction\")\n",
    "print(\" 0: Random prediction\")\n",
    "print(\"-1: Perfect inverse prediction\")\n",
    "print(\"Especially useful for imbalanced datasets\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 2. MULTI-CLASS CLASSIFICATION METRICS\n",
    "# ============================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"\\n\\n MULTI-CLASS CLASSIFICATION METRICS\")\n",
    "print(\"=\" * 45)\n",
    "\n",
    "# Generate fake multi-class classification data\n",
    "n_classes = 4\n",
    "class_names = ['Class A', 'Class B', 'Class C', 'Class D']\n",
    "\n",
    "# True labels\n",
    "y_true_multi = np.random.choice(n_classes, n_samples, p=[0.4, 0.3, 0.2, 0.1])\n",
    "\n",
    "# Predicted probabilities (simulating a decent model)\n",
    "y_prob_multi = np.random.dirichlet([2, 1.5, 1, 0.5], n_samples)\n",
    "# Add some bias towards correct predictions\n",
    "for i in range(n_samples):\n",
    "    y_prob_multi[i, y_true_multi[i]] *= np.random.uniform(1.5, 3.0)\n",
    "    y_prob_multi[i] /= y_prob_multi[i].sum()  # Normalize\n",
    "\n",
    "# Predicted labels\n",
    "y_pred_multi = np.argmax(y_prob_multi, axis=1)\n",
    "\n",
    "print(f\"Dataset: {n_samples} samples, {n_classes} classes\")\n",
    "print(\"Class distribution:\")\n",
    "for i, name in enumerate(class_names):\n",
    "    true_count = sum(y_true_multi == i)\n",
    "    pred_count = sum(y_pred_multi == i)\n",
    "    print(f\"  {name}: True={true_count:3d} ({true_count/n_samples:.1%}), \"\n",
    "          f\"Pred={pred_count:3d} ({pred_count/n_samples:.1%})\")\n",
    "\n",
    "# %%\n",
    "# 2.1 Multi-class Confusion Matrix\n",
    "print(\"\\n Multi-class Confusion Matrix\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "cm_multi = confusion_matrix(y_true_multi, y_pred_multi)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(\"Rows = True labels, Columns = Predicted labels\")\n",
    "\n",
    "# Create a DataFrame for better visualization\n",
    "cm_df = pd.DataFrame(cm_multi, index=class_names, columns=class_names)\n",
    "print(cm_df)\n",
    "\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "sns.heatmap(cm_multi, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=class_names, yticklabels=class_names)\n",
    "plt.title('Multi-class Confusion Matrix')\n",
    "plt.ylabel('True Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "\n",
    "# %%\n",
    "# 2.2 Macro vs Micro Averaging\n",
    "print(\"\\n⚖️ Macro vs Micro Averaging\")\n",
    "print(\"-\" * 28)\n",
    "\n",
    "# Calculate metrics with different averaging strategies\n",
    "accuracy_multi = accuracy_score(y_true_multi, y_pred_multi)\n",
    "precision_macro = precision_score(y_true_multi, y_pred_multi, average='macro')\n",
    "precision_micro = precision_score(y_true_multi, y_pred_multi, average='micro')\n",
    "precision_weighted = precision_score(y_true_multi, y_pred_multi, average='weighted')\n",
    "\n",
    "recall_macro = recall_score(y_true_multi, y_pred_multi, average='macro')\n",
    "recall_micro = recall_score(y_true_multi, y_pred_multi, average='micro')\n",
    "recall_weighted = recall_score(y_true_multi, y_pred_multi, average='weighted')\n",
    "\n",
    "f1_macro = f1_score(y_true_multi, y_pred_multi, average='macro')\n",
    "f1_micro = f1_score(y_true_multi, y_pred_multi, average='micro')\n",
    "f1_weighted = f1_score(y_true_multi, y_pred_multi, average='weighted')\n",
    "\n",
    "print(f\"Overall Accuracy: {accuracy_multi:.4f}\")\n",
    "print(\"\\nPrecision:\")\n",
    "print(f\"  Macro:    {precision_macro:.4f} (unweighted mean)\")\n",
    "print(f\"  Micro:    {precision_micro:.4f} (global average)\")\n",
    "print(f\"  Weighted: {precision_weighted:.4f} (weighted by support)\")\n",
    "\n",
    "print(\"\\nRecall:\")\n",
    "print(f\"  Macro:    {recall_macro:.4f}\")\n",
    "print(f\"  Micro:    {recall_micro:.4f}\")\n",
    "print(f\"  Weighted: {recall_weighted:.4f}\")\n",
    "\n",
    "print(\"\\nF1-Score:\")\n",
    "print(f\"  Macro:    {f1_macro:.4f}\")\n",
    "print(f\"  Micro:    {f1_micro:.4f}\")\n",
    "print(f\"  Weighted: {f1_weighted:.4f}\")\n",
    "\n",
    "# Per-class metrics\n",
    "print(\"\\nPer-class Metrics:\")\n",
    "precision_per_class = precision_score(y_true_multi, y_pred_multi, average=None)\n",
    "recall_per_class = recall_score(y_true_multi, y_pred_multi, average=None)\n",
    "f1_per_class = f1_score(y_true_multi, y_pred_multi, average=None)\n",
    "\n",
    "for i, name in enumerate(class_names):\n",
    "    print(f\"  {name}: Precision={precision_per_class[i]:.3f}, \"\n",
    "          f\"Recall={recall_per_class[i]:.3f}, F1={f1_per_class[i]:.3f}\")\n",
    "\n",
    "# %%\n",
    "# 2.3 Cohen's Kappa\n",
    "print(\"\\n Cohen's Kappa\")\n",
    "print(\"-\" * 15)\n",
    "\n",
    "kappa = cohen_kappa_score(y_true_multi, y_pred_multi)\n",
    "print(f\"Cohen's Kappa: {kappa:.4f}\")\n",
    "print(\"Kappa measures inter-rater agreement\")\n",
    "print(\"Accounts for agreement by chance\")\n",
    "print(\"Interpretation:\")\n",
    "print(\"  < 0.00: Poor agreement\")\n",
    "print(\"  0.00-0.20: Slight agreement\")\n",
    "print(\"  0.21-0.40: Fair agreement\") \n",
    "print(\"  0.41-0.60: Moderate agreement\")\n",
    "print(\"  0.61-0.80: Substantial agreement\")\n",
    "print(\"  0.81-1.00: Almost perfect agreement\")\n",
    "\n",
    "# %%\n",
    "# 2.4 Multi-class ROC Curves\n",
    "print(\"\\n Multi-class ROC Analysis\")\n",
    "print(\"-\" * 25)\n",
    "\n",
    "# Binarize the labels for ROC calculation\n",
    "y_true_binarized = label_binarize(y_true_multi, classes=range(n_classes))\n",
    "\n",
    "# Calculate ROC curve and AUC for each class\n",
    "fpr_multi = {}\n",
    "tpr_multi = {}\n",
    "roc_auc_multi = {}\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "colors = ['blue', 'red', 'green', 'orange']\n",
    "\n",
    "for i in range(n_classes):\n",
    "    fpr_multi[i], tpr_multi[i], _ = roc_curve(y_true_binarized[:, i], y_prob_multi[:, i])\n",
    "    roc_auc_multi[i] = auc(fpr_multi[i], tpr_multi[i])\n",
    "    plt.plot(fpr_multi[i], tpr_multi[i], color=colors[i], lw=2,\n",
    "             label=f'{class_names[i]} (AUC = {roc_auc_multi[i]:.3f})')\n",
    "\n",
    "plt.plot([0, 1], [0, 1], 'k--', lw=1)\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Multi-class ROC Curves')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "print(\"Per-class AUC-ROC scores:\")\n",
    "for i, name in enumerate(class_names):\n",
    "    print(f\"  {name}: {roc_auc_multi[i]:.4f}\")\n",
    "\n",
    "# %%\n",
    "# Classification Report\n",
    "print(\"\\n Detailed Classification Report\")\n",
    "print(\"-\" * 32)\n",
    "print(classification_report(y_true_multi, y_pred_multi, target_names=class_names))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# =============================================================================\n",
    "# 3. REGRESSION METRICS\n",
    "# ============================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\\n REGRESSION METRICS\")\n",
    "print(\"=\" * 25)\n",
    "\n",
    "# Generate fake regression data\n",
    "y_true_reg = np.random.normal(50, 15, n_samples)  # True values\n",
    "noise = np.random.normal(0, 5, n_samples)  # Prediction noise\n",
    "y_pred_reg = y_true_reg + noise + np.random.normal(2, 1, n_samples)  # Slight bias\n",
    "\n",
    "print(f\"Dataset: {n_samples} samples\")\n",
    "print(f\"True values range: [{y_true_reg.min():.2f}, {y_true_reg.max():.2f}]\")\n",
    "print(f\"Predicted values range: [{y_pred_reg.min():.2f}, {y_pred_reg.max():.2f}]\")\n",
    "\n",
    "# %%\n",
    "# 3.1 Mean Squared Error and RMSE\n",
    "print(\"\\nMean Squared Error (MSE) and RMSE\")\n",
    "print(\"-\" * 35)\n",
    "\n",
    "mse = mean_squared_error(y_true_reg, y_pred_reg)\n",
    "rmse = np.sqrt(mse)\n",
    "\n",
    "print(f\"MSE:  {mse:.4f}\")\n",
    "print(f\"RMSE: {rmse:.4f}\")\n",
    "print(\"MSE penalizes large errors more heavily\")\n",
    "print(\"RMSE is in the same units as the target variable\")\n",
    "\n",
    "# %%\n",
    "# 3.2 Mean Absolute Error\n",
    "print(\"\\nMean Absolute Error (MAE)\")\n",
    "print(\"-\" * 25)\n",
    "\n",
    "mae = mean_absolute_error(y_true_reg, y_pred_reg)\n",
    "print(f\"MAE: {mae:.4f}\")\n",
    "print(\"MAE is less sensitive to outliers than MSE/RMSE\")\n",
    "print(\"Represents average absolute difference\")\n",
    "\n",
    "# %%\n",
    "# 3.3 R-squared (Coefficient of Determination)\n",
    "print(\"\\n📊 R-squared (R²)\")\n",
    "print(\"-\" * 15)\n",
    "\n",
    "r2 = r2_score(y_true_reg, y_pred_reg)\n",
    "print(f\"R²: {r2:.4f}\")\n",
    "print(\"R² represents proportion of variance explained\")\n",
    "print(\"R² = 1: Perfect predictions\")\n",
    "print(\"R² = 0: As good as predicting the mean\")\n",
    "print(\"R² < 0: Worse than predicting the mean\")\n",
    "\n",
    "# Calculate components for understanding\n",
    "ss_res = np.sum((y_true_reg - y_pred_reg) ** 2)  # Residual sum of squares\n",
    "ss_tot = np.sum((y_true_reg - np.mean(y_true_reg)) ** 2)  # Total sum of squares\n",
    "r2_manual = 1 - (ss_res / ss_tot)\n",
    "print(f\"R² = 1 - (SS_res / SS_tot) = 1 - ({ss_res:.2f} / {ss_tot:.2f}) = {r2_manual:.4f}\")\n",
    "\n",
    "# %%\n",
    "# 3.4 Mean Absolute Percentage Error\n",
    "print(\"\\n📊 Mean Absolute Percentage Error (MAPE)\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# Avoid division by zero\n",
    "y_true_nonzero = y_true_reg[y_true_reg != 0]\n",
    "y_pred_nonzero = y_pred_reg[y_true_reg != 0]\n",
    "\n",
    "mape = np.mean(np.abs((y_true_nonzero - y_pred_nonzero) / y_true_nonzero)) * 100\n",
    "print(f\"MAPE: {mape:.2f}%\")\n",
    "print(\"MAPE gives percentage error relative to true values\")\n",
    "print(\"Useful for understanding relative prediction accuracy\")\n",
    "\n",
    "# %%\n",
    "# 3.5 Regression Visualization\n",
    "print(\"\\n📈 Regression Analysis Visualization\")\n",
    "print(\"-\" * 33)\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "# Actual vs Predicted scatter plot\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.scatter(y_true_reg, y_pred_reg, alpha=0.5, s=20)\n",
    "plt.plot([y_true_reg.min(), y_true_reg.max()], [y_true_reg.min(), y_true_reg.max()], \n",
    "         'r--', lw=2, label='Perfect Prediction')\n",
    "plt.xlabel('True Values')\n",
    "plt.ylabel('Predicted Values')\n",
    "plt.title('Actual vs Predicted')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Residuals plot\n",
    "residuals = y_true_reg - y_pred_reg\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.scatter(y_pred_reg, residuals, alpha=0.5, s=20)\n",
    "plt.axhline(y=0, color='r', linestyle='--')\n",
    "plt.xlabel('Predicted Values')\n",
    "plt.ylabel('Residuals')\n",
    "plt.title('Residuals Plot')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Residuals histogram\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.hist(residuals, bins=30, alpha=0.7, edgecolor='black')\n",
    "plt.xlabel('Residuals')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Residuals Distribution')\n",
    "plt.axvline(x=0, color='r', linestyle='--')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Summary of regression metrics\n",
    "print(f\"\\n📋 Regression Metrics Summary:\")\n",
    "print(f\"MSE:  {mse:.4f}\")\n",
    "print(f\"RMSE: {rmse:.4f}\")\n",
    "print(f\"MAE:  {mae:.4f}\")\n",
    "print(f\"R²:   {r2:.4f}\")\n",
    "print(f\"MAPE: {mape:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# =============================================================================\n",
    "# 4. Additional Topics\n",
    "# ============================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\\nAdditional Topics\")\n",
    "print(\"=\" * 20)\n",
    "\n",
    "# %%\n",
    "# 4.1 Class Imbalance Impact\n",
    "print(\"\\n⚖️ Class Imbalance Considerations\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "# Create highly imbalanced dataset\n",
    "y_imbalanced = np.random.binomial(1, 0.05, n_samples)  # Only 5% positive\n",
    "y_pred_imbalanced = np.random.binomial(1, 0.05, n_samples)  # Naive predictor\n",
    "\n",
    "print(f\"Imbalanced dataset: {y_imbalanced.mean():.1%} positive class\")\n",
    "\n",
    "acc_imb = accuracy_score(y_imbalanced, y_pred_imbalanced)\n",
    "prec_imb = precision_score(y_imbalanced, y_pred_imbalanced, zero_division=0)\n",
    "rec_imb = recall_score(y_imbalanced, y_pred_imbalanced)\n",
    "f1_imb = f1_score(y_imbalanced, y_pred_imbalanced, zero_division=0)\n",
    "\n",
    "print(f\"Accuracy:  {acc_imb:.4f} (Can be misleading!)\")\n",
    "print(f\"Precision: {prec_imb:.4f}\")\n",
    "print(f\"Recall:    {rec_imb:.4f}\")\n",
    "print(f\"F1-Score:  {f1_imb:.4f}\")\n",
    "print(\"\\n💡 For imbalanced data, focus on:\")\n",
    "print(\"   - Precision, Recall, F1-Score\")\n",
    "print(\"   - AUC-ROC and AUC-PR\")\n",
    "print(\"   - Matthews Correlation Coefficient\")\n",
    "\n",
    "# %%\n",
    "# 4.2 Metric Selection Guidelines\n",
    "print(\"\\n🎯 Metric Selection Guidelines\")\n",
    "print(\"-\" * 28)\n",
    "\n",
    "guidelines = {\n",
    "    \"Binary Classification\": {\n",
    "        \"Balanced classes\": [\"Accuracy\", \"F1-Score\", \"AUC-ROC\"],\n",
    "        \"Imbalanced classes\": [\"Precision\", \"Recall\", \"F1-Score\", \"AUC-PR\", \"MCC\"],\n",
    "        \"Cost-sensitive\": [\"Precision (minimize FP)\", \"Recall (minimize FN)\"],\n",
    "        \"Probability calibration\": [\"Log Loss\", \"Brier Score\"]\n",
    "    },\n",
    "    \"Multi-class Classification\": {\n",
    "        \"Balanced classes\": [\"Accuracy\", \"Macro F1\", \"Cohen's Kappa\"],\n",
    "        \"Imbalanced classes\": [\"Weighted F1\", \"Cohen's Kappa\", \"Per-class metrics\"],\n",
    "        \"Many classes\": [\"Top-k Accuracy\", \"Mean Reciprocal Rank\"]\n",
    "    },\n",
    "    \"Regression\": {\n",
    "        \"General purpose\": [\"RMSE\", \"MAE\", \"R²\"],\n",
    "        \"Outlier robust\": [\"MAE\", \"Huber Loss\"],\n",
    "        \"Relative errors\": [\"MAPE\", \"SMAPE\"],\n",
    "        \"Distribution shape\": [\"Quantile Loss\"]\n",
    "    }\n",
    "}\n",
    "\n",
    "for task, scenarios in guidelines.items():\n",
    "    print(f\"\\n{task}:\")\n",
    "    for scenario, metrics in scenarios.items():\n",
    "        print(f\"  {scenario}: {', '.join(metrics)}\")\n",
    "\n",
    "# %%\n",
    "# 4.3 Cross-Validation Considerations\n",
    "print(\"\\n🔄 Cross-Validation and Metric Stability\")\n",
    "print(\"-\" * 38)\n",
    "\n",
    "# Simulate CV results for different metrics\n",
    "cv_folds = 5\n",
    "cv_results = {\n",
    "    'Accuracy': np.random.normal(0.85, 0.03, cv_folds),\n",
    "    'F1-Score': np.random.normal(0.82, 0.05, cv_folds),\n",
    "    'AUC-ROC': np.random.normal(0.91, 0.02, cv_folds),\n",
    "    'Precision': np.random.normal(0.80, 0.08, cv_folds),\n",
    "    'Recall': np.random.normal(0.84, 0.06, cv_folds)\n",
    "}\n",
    "\n",
    "print(\"Cross-Validation Results (5-fold):\")\n",
    "print(\"Metric      Mean    Std     Min     Max\")\n",
    "print(\"-\" * 40)\n",
    "for metric, scores in cv_results.items():\n",
    "    print(f\"{metric:10} {scores.mean():.3f}  {scores.std():.3f}  {scores.min():.3f}  {scores.max():.3f}\")\n",
    "\n",
    "print(\"\\n💡 Consider metric stability across folds!\")\n",
    "print(\"   High standard deviation indicates unstable performance\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
